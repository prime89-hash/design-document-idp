Intelligent Document Processing (IDP) – Low Level Design (LLD)
Use Case 3 – Evoke
________________________________________
1. Introduction
The Intelligent Document Processing (IDP) system automates classification, language detection, and extraction of structured fields from documents using AWS Bedrock Data Automation (BDA) and a fully serverless architecture.
All development and testing are done using synthetic (non PII) documents, ensuring compliance and safety.
Prompts, extraction rules, confidence thresholds, and model selections are configurable, stored in S3, and updated via CI/CD — allowing continuous improvements without redeploying code.
This LLD describes the complete technical implementation, infrastructure design, configuration strategy, prompt testing workflow, and promotion model across environments.
________________________________________
2. High-Level Architecture Overview
The IDP solution is built entirely on AWS serverless components, with no VPC required.
Runtime Architecture Components:
•	API Gateway – creates pre signed URLs for document upload
•	Amazon S3 (Input, Output, Config, Prompts)
•	EventBridge – triggers workflow
•	Step Functions – main orchestration of all steps
•	Lambda Functions – validation, BDA calls, post-processing, callbacks
•	Bedrock Data Automation (BDA) – classification, extraction, OCR
•	DynamoDB – job tracking
•	SQS (DLQ) – failure handling
•	CloudWatch + New Relic – logs, metrics, monitoring
Prompt Lab (Experimental Environment)
Used strictly for:
•	Prompt design
•	Testing
•	Evaluation
•	Synthetic dataset experimentation
________________________________________
3. Environments
Environment	Purpose
Experimental Platform	Prompt testing, BDA experimentation, synthetic data evaluation
DEV	Developer testing using synthetic data
Non-Prod 	
PROD	
________________________________________
4. End-to-End Processing Flow (Runtime)
Step 1 — Upload Initiation
1.	Client calls POST /idp/upload/initiate via API Gateway.
2.	Lambda generates: 
o	jobId
o	pre signed S3 upload URL
3.	Metadata stored in DynamoDB.
________________________________________
Step 2 — Document Upload
4.	Client uploads document using pre signed URL → S3 Input bucket.
5.	EventBridge detects the new object and triggers Step Functions.
________________________________________
Step 3 — Workflow Orchestration
6.	Step Functions loads:
o	Document-type config from S3 Config
o	Prompt file from S3 Prompts
7.	Workflow steps:
o	Validate file (size, extension, path correctness)
o	Preprocessing / OCR (if required)
o	Call BDA for classification + extraction
o	Validate confidence thresholds
o	Format result JSON
o	Store result in S3 Output
o	Update DynamoDB status
________________________________________
Step 4 — Callback
8.	Callback Lambda notifies client with: 
o	jobStatus
o	output location
o	confidence scores
o	document type
________________________________________
Step 5 — Logging & Monitoring
9.	CloudWatch stores logs.
10.	New Relic dashboards monitor: 
o	Failures
o	Latency
o	Drift in model quality
o	DLQ depth
________________________________________


5.Component-Level LLD
     Key Design Principles
•	Event-Driven Architecture: Leverages AWS EventBridge for loose coupling and scalability
•	Serverless-First: Uses Lambda functions and Step Functions for cost optimization
•	Configuration-Driven: Supports dynamic parameter changes without code deployment
•	Security by Design: Implements comprehensive security controls and data protection
•	Observability: Built-in monitoring, logging, and alerting capabilities
________________________________________
5.1 S3 Buckets
Input Bucket
Used for document uploads.
Structure:
idp-input/<tenant>/<country>/<doctype>/<uuid>.<ext>
Output Bucket
Stores structured extraction results.
idp-output/<tenant>/<jobId>/result.json
Config Bucket
Stores JSON configs for each document type:
•	fields
•	thresholds
•	model selection
•	blueprint IDs
•	language hints
Prompts Bucket
Stores prompt files for BDA:
•	/prompts/<doctype>/system.txt
•	optional few shot examples
________________________________________

5.2 DynamoDB Table
Table: idp_jobs
Attribute	Description
jobId	Primary key
tenant, country	Routing info
status	RECEIVED / PROCESSING / COMPLETED / FAILED
docType	Detected or configured type
language	Detected language
confidence	extraction + classification scores
s3InputKey, s3OutputKey	File references
configVersion	Version used during execution
timestamps	Start/End/Callback times
________________________________________
5.3 Lambda Functions
Upload Init Lambda
•	Validate request
•	Generate pre signed URL
•	Store metadata in DynamoDB
Orchestrator Lambda
•	Load config & prompt
•	Call Bedrock BDA
•	Validate response
•	Format output
Callback Lambda
•	Send callback HTTP POST
•	Retry on failure
Error Manager Lambda
•	Read from SQS DLQ
•	Log, alert, and flag issues
________________________________________
5.4 Step Functions Orchestration
Purpose: Orchestrate document processing pipeline with comprehensive error handling and monitoring
Step Functions Workflow States
Processing Flow:
1.	INIT/Correlate - Initialize processing context and generate correlation IDs
2.	Document Validation - Verify file format, size, and accessibility
3.	BDA Processing - Single call to Bedrock Data Automation for complete document processing
4.	Confidence Check - Evaluate if results meet quality thresholds
5.	Quality Evaluation - LLM-based assessment of extraction accuracy
6.	Output Generation - Create structured JSON output
7.	Resource Cleanup - Remove temporary files and update status
8.	Notification - Inform client application of completion

Error Handling:
•	Automatic Retry - 3 attempts with exponential backoff for transient failures
•	Manual Review Queue - Low confidence results routed for human review
•	Dead Letter Queue - Failed processing items for investigation
•	Error Notifications - Client applications notified of processing failures
Quality Gates:
•	Confidence Threshold - Minimum 85% confidence for automatic processing
•	Format Validation - Only PDF, JPG, JPEG, TIFF files accepted
•	Size Limits - Maximum 10MB file size
•	Processing Timeout - 5-minute maximum processing time
________________________________________
5.5 Document Processing Engine
Purpose: Core component for document classification and extraction
Sub-Components:
5.5.1 Language Detection Service
•	Technology: Bedrock Data Automation with language detection blueprints
•	Input: Raw document uploaded to S3
•	Output: Language code with confidence score via BDA
•	Threshold: Minimum 95% confidence for language detection
5.5.2 Document Classification Service
•	Technology: Bedrock Data Automation with classification blueprints
•	Input: Document and detected language from BDA
•	Output: Document type classification with confidence score
•	Supported Types: ID, POA, SOF, PH/ID, MFT, Legal Claims
5.5.3 Field Extraction Service
•	Technology: Bedrock Data Automation with extraction blueprints and schemas
•	Input: Classified document and extraction schema
•	Output: Structured JSON with extracted fields and confidence scores
•	Schema Management: BDA blueprints contain predefined schemas for each document type

________________________________________
6. Configuration Strategy
Purpose: Dynamic configuration for document processing parameters
Configuration Structure:
 
Configuration Deployment:
•	Stored in S3 with versioning enabled
•	Lambda function validates configuration changes
•	Automatic rollback on validation failures
•	Configuration change triggers pipeline deployment
Config JSON Structure
•	doctype
•	required fields
•	optional fields
•	confidence thresholds
•	blueprint name
•	LLM model ID
•	language hints
•	post-processing rules
Version Control
•	Source of truth stored in GitLab
•	CI/CD promotes to S3
•	Each config has semantic version + checksum
•	Runtime logs record configVersion
________________________________________
7. Prompt Testing & Management (Experimental Environment)
Prompt Testing Cycle
1.	SME designs prompts in Prompt Lab
2.	Developer loads synthetic datasets
3.	BDA prompt tests run manually or via Jupyter/SDK
4.	Evaluate:
o	accuracy
o	extraction quality
o	hallucination risk
o	confidence calibration
o	multilingual performance
5.	SME reviews results
6.	Iterate until metrics meet thresholds
Assets Produced
•	system prompt
•	few shot examples
•	extraction schema
•	evaluation report
•	version number
Promotion
•	Commit prompt + config to GitLab
•	CI/CD validates & pushes to S3 Config & S3 Prompts
________________________________________
8. Pipeline Architecture Overview
The CI/CD strategy implements three separate pipelines to enable independent deployment cycles and reduce blast radius of changes:
1.	Infrastructure Pipeline - Manages AWS resources and networking
2.	Application Pipeline - Handles Lambda functions and application code
3.	Prompt+Config Pipeline - Manages BDA blueprints and configuration
Pipeline 1: Infrastructure Pipeline
Purpose: Deploy and manage AWS infrastructure using Terraform
 
GitLab stages:
  - validate
  - plan
  - security-scan
  - deploy-dev
  - deploy-nonprod
Infrastructure Components Managed:
•	API Gateway with security policies and rate limiting
•	Lambda functions with appropriate IAM roles
•	S3 buckets with versioning and encryption
•	DynamoDB tables with backup configuration
•	IAM roles and policies with least privilege
•	EventBridge rules and targets
•	Step Functions state machines
•	CloudWatch log groups and alarms
•	New Relic integration resources
•	Secrets Manager for secure credential storage

________________________________________
Pipeline 2: Application Pipeline
Purpose: Build, test, and deploy Lambda function code and container images
Trigger Conditions:
•	Changes to src/ directory
•	Changes to requirements.txt or package.json
•	Manual pipeline execution
stages:
  - test
  - build
  - security-scan
  - deploy-dev
  - integration-test
  - deploy-nonprod
________________________________________
Pipeline 3: Prompt+Config Pipeline
Purpose: Manage BDA blueprints, prompts, and system configuration
Trigger Conditions:
•	Changes to config/ directory
•	Changes to prompts/ directory
•	Changes to blueprints/ directory
•	Manual pipeline execution
stages:
  - validate
  - test-synthetic
  - deploy-dev
  - evaluate-quality
  - deploy-nonprod________________________________________
Testing Strategy
1. Unit Testing
Scope: Individual Lambda functions and utility modules
Framework: pytest with mocking for AWS services
Coverage Requirements: Minimum 80% code coverage
Test Categories:
•	Function input/output validation
•	Error handling scenarios
•	Configuration parsing logic
•	Data transformation functions
2. Integration Testing
Scope: End-to-end workflow testing with AWS services
Test Environment: Dedicated dev environment with synthetic data
Test Scenarios:
•	Document upload and BDA processing workflow
•	Configuration change propagation to BDA blueprints
•	Error handling and retry mechanisms
•	Performance under load with BDA API calls
3. Synthetic Data Testing
Purpose: Validate processing accuracy with controlled test data
Test Data Generation:
•	Programmatically generated documents for each type
•	Multiple languages and layouts
•	Edge cases and boundary conditions
Validation Criteria:
•	Classification accuracy ≥ 95%
•	Field extraction accuracy ≥ 90%
•	Processing time ≤ 30 seconds per document
4. LLM Evaluation Testing
Purpose: Automated quality assessment using LLM-based evaluation
Evaluation Process:
1.	Process synthetic documents through pipeline
2.	Compare results against ground truth using LLM evaluator
3.	Generate quality scores and detailed reports
4.	Flag discrepancies for manual review
Quality Thresholds:
•	Overall quality score ≥ 0.85
•	Classification accuracy ≥ 0.95
•	Field-level precision ≥ 0.90
•	Field-level recall ≥ 0.85
5. Performance Testing
Load Testing:
•	Concurrent document processing: 100 documents/minute
•	Peak load handling: 500 documents/minute for 10 minutes
•	Memory usage monitoring for Lambda functions
Stress Testing:
•	Large document processing (up to 10MB)
•	Complex document layouts
•	Multiple language processing
Security Considerations
Data Protection
1.	Encryption:
o	S3 buckets encrypted with KMS keys
o	DynamoDB encryption at rest
o	Lambda environment variables encrypted
2.	Access Control:
o	IAM roles with least privilege principle
o	Resource-based policies for cross-service access
o	VPC endpoints for private service communication
3.	Data Lifecycle:
o	Automatic document deletion after processing
o	Log retention policies (1 year for AWS, 30 days for application)
o	No PII data in logs or metrics
Network Security
1.	Serverless Security:
o	Lambda functions in AWS-managed secure environment
o	No custom VPC required for this use case
o	Service-to-service communication via AWS backbone
2.	API Security:
o	API Gateway with API keys and rate limiting
o	HTTPS enforcement for all endpoints
o	CORS configuration for web clients
o	AWS WAF for additional protection (optional)
3.	Service Integration Security:
o	IAM roles for service-to-service authentication
o	Resource-based policies for fine-grained access
o	AWS PrivateLink for on-premises connectivity (if needed)

